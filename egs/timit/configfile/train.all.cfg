#[common]
# method can be one of the followings - train,predict,load
mode = train

# use_cpu=True means use cpu as context, and this will ignore use_gpu, otherwise set it false
use_cpu = False
# define the id of gpu to use, will be ignored if use cpu, for example: use_gpu=0,1,2
use_gpu = 0,1,2

# checkpoint prefix, check point will be saved under checkpoints folder with prefix
checkpoint_dir = ./checkpoints.swbd.p2w.ce.1wb_1lex.lstm_l4_c700_p256_nobatchnorm
prefix = swbd.p2w.ce.1wb_1lex.lstm_l4_c700_p256_nobatchnorm

# when mode is load or predict, model will be loaded from the file name with model_file under checkpoints
model_file = swbd.p2w.ce.1wb_1lex.lstm_l4_c700_p256_nobatchnorm-0080
batch_size = 120

# log will be saved by the log_filename
log_file_dir = ./log
log_filename = train.swbd.p2w.ce.1wb_1lex.lstm_l4_c700_p256_nobatchnorm.log

# checkpoint set n to save checkpoints after n epoch
save_checkpoint_every_n_epoch = 1

# if random_seed is -1 then it gets random seed from timestamp
mx_random_seed = -1
random_seed = -1
kvstore_option = device

# [data]
feat_dim = 47
train_csv = ./resources/train_swbd.csv
dev_csv = ./resources/dev_swbd.csv
test_csv = ./resources/dev_swbd.csv
dict_csv = ./resources/vocab.csv

# [arch]
default_bucket_key = 800
buckets = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 60, 70, 80, 90, 100, 120, 140, 160, 180, 200, 220, 240, 260, 280, 300]
num_rnn_layer = 4
num_hidden_rnn_list = [700, 700, 700, 700]
num_hidden_proj = 256

num_rear_fc_layers = 0
num_hidden_rear_fc_list = []
act_type_rear_fc_list = []

#network: lstm, bilstm, gru, bigru
rnn_type = lstm
is_batchnorm = False

# [train]
num_epoch = 60
learning_rate = 0.0001
# constant learning rate annealing by factor
learning_rate_annealing = 1.4
initializer = Xavier
init_scale = 2
factor_type = in
# show progress every nth batches 96:5000/300,128:5000/300
train_show_every = 1500
dev_show_every = 150
save_optimizer_states = True
enable_logging_train_metric = True
enable_logging_validation_metric = True

# [load]
load_optimizer_states = True

# [optimizer]
optimizer = adam
# define parameters for optimizer
# optimizer_params_dictionary should use " not ' as string wrapper
# sgd/nag
# optimizer_params_dictionary={"momentum":0.9}
# dcasgd
# optimizer_params_dictionary={"momentum":0.9, "lamda":1.0}
# adam
optimizer_params_dictionary={"beta1":0.9,"beta2":0.999}
# adagrad
# optimizer_params_dictionary={"eps":1e-08}
# rmsprop
# optimizer_params_dictionary={"gamma1":0.9, "gamma2":0.9,"epsilon":1e-08}
# adadelta
# optimizer_params_dictionary={"rho":0.95, "epsilon":1e-08}
# set to 0 to disable gradient clipping
clip_gradient = 0
weight_decay = 0.
